We propose to proof-check inner skolemization by injecting an explicit
cut formula and cut inference rule.



The framework we introduced applies right away to the certification of
proofs that make use of Skolem terms introduced with outermost
skolemization.
%
In this section we illustrate the additions that need to be done to
the \LKF kernel in order to distinguish between client and kernel
terms, and we will describe how to modify accordingly an FPC in order
to be able to check proof evidence obtained using skolemization.

We use them by putting a client term in the first argument, and a
kernel term in the second argument of the \texttt{copy} predicate. We
obtain in this way a method to translate between them.
%
Kernel and client terms agree on all terms in the signature
$\Sigma_0$, and so copy clauses in the style of those in
figure~\ref{fig:copyc} are introduced for these terms; the differences
will arise from those terms that are used by the client but are not in
the signature of the theorem, such as those from the signature
$\sksig$ of Skolem terms.
%
As we have noted, these terms take the role of names for
eigenvariables; therefore when the kernel creates a new eigenvariable
it will need to add to the program a copy clause with the
eigenvariable as the second argument, where the first argument should
be matched by the correct client term.


% TODO: Spaces are highlighted in this listing, possibly due to missing
% showspaces=false and showstringspaces=false  options to the listings package

We only need to touch the kernel in two simple ways in order to
implement this mechanism, as illustrated in
figure~\ref{fig:newkernel}.
%
The first is the inclusion of a new clerk \texttt{allCx} for the
universal quantifier, that will have an extra argument for the client
term to be associated with the created eigenvariable and that will
create the new copy-clause.
%
The second is the inclusion of a call to \texttt{copy} in the body of
the \texttt{someE} expert, in order to translate the term provided by
the client as witness for the existential to a kernel term.

It is clear that any FPC built to run on the traditional kernel will
run unmodified on a kernel updated with support for client and kernel
terms: \texttt{allCx} is never used, client and kernel terms collapse
to the same entity and so the behavior of \texttt{someE} is the same
as the old one.
%
The interest now is to see how we can patch an existing FPC to use the
new kernel in order check proof evidence that employs outer
skolemization.

Consider any FPC for a kernel implementing \LK. We are in a setting
where the client hands us a formula $F$, and a proof evidence $\Xi$
for its outer skolemization $F^\prime$. We immediately note that since
$F^\prime$ does not contain strong quantifiers, the direct check for
$\Xi$ against the skolemized formula will not involve any calls to the
\texttt{allC} clerk.

In order to account for the strong quantifiers that are present in
$F$, we will make use of the new \texttt{allCx} clerk. Note that this
clerk is called when we meet a strong quantifier in the checked
formula; in that moment, the kernel will need to create a new
eigenvariable, but it still does not know what is the client term
corresponding to it.
%
Therefore, we need to add to the initial FPC a predicate for
\texttt{allCx} with a logic variable supplied as a client term.

\emph{TODO: Explain how the kernel will figure out the correct term?}

Thus, thanks to the use of the new kernel and the addition of a simple
predicate definition to the original FPC, we are able to check the
skolemized proof evidence $\Xi$ against the original formula $F$.

In Section~\ref{sec:implementation} we will apply this technique to
two FPCs and obtain new versions with support for skolemized
certificates.







\subsection{Unused parts from the outline}
The soundness of outermost skolemization: if the proof evidence for
sk(B) is *cut-free* (meaning what?) then we can translate that proof
to a cut-free proof of B.  (The proposal for *cut-free* proof evidence
means that the kernel does not invoke the cut-expert).

NB In a cut-free setting, only terms need to be translated (client->
kernel).  If we also have cut-formulas, we would need to translate
them as well?

If we have checked proof evidence $\Xi$ for $sk(B)$ and $\theta$ is
\emph{any}(higher-order) substitution whose domain is only Skolem
functions, then $\theta(\Xi)$ will check as proof evidence of $\theta
(sk(B))$.

Thus, if $Prog \vdash \mathtt{check} \, B \, \Xi$ and if $\theta$ is a
substitution for Skolem function symbols, then $Prog \vdash
\mathtt{check}\, (\theta B) \, (\theta \Xi)$.The converse is not true.

When you take the instance via $\theta$, the proof
checking/reconstruction process will increase in nondeterminism.  If
$\theta$ throws away information, there is a cost in reconstructing
it.

Thus, if the proof evidence uses outer-most skolemization, then
changing it to be innermost will still proof-check.  (This is
immediate.)  Similarly, renaming all Skolem functions to have the same
name will also result in checkable proof evidence.

It is possible to ``simplify'' Skolem terms.  For example, we can
replace $sk3$ with, say, $\lambda x.\, \lambda y.\, \lambda z.\,
(sk2\, y\, z)$, that is, replace a three argument Skolem function with
a two-argument Skolem function (discarding an argument).

This aspect of ``instantiating Skolem functions'' sends checkable
proof evidence to checkable proof evidence seems ``new''.  Also, the
trade-off with nondeterminism in proof checking.

Of course, when one is doing theorem proving (not proof checking),
dropping too many arguments during skolemization can be unsound.



\begin{figure}
  \centering
  \begin{lstlisting}
async Cert [all B|Rest] :-
  allCx Cert Cert' T, pi w\ (copy T w) => async (Cert' w) [B w|Rest].

sync Cert (some B) :-
  someE  Cert Cert' T, copy T T', sync Cert' (B T').
sync Cert (some B) :-
  someEx Cert Cert', sync Cert' (B T).
  \end{lstlisting}
\caption{Additions to the kernel for handling of client and kernel terms}
  \label{fig:newkernel}
\end{figure}


%
(3) The predicate $\WF t$ mentioned in Figure~\ref{fig:lkf-rules} can
be defined in an analogous fashion based on the signature and the 



% {\color{red} DM: Maybe this section should disappear or be
%   incorporated into the material on \LKF and FPCs.}
%
% We shall use a proof system for classical first-order logic that is
% similar to the Gentzen-Sch\"utte-style (a.k.a. Tait-style) proof
% system  GS[1,2,3] of \cite{troelstra00book} in that all formulas are
% assumed to be in negation normal form and that sequents are one-sided
% only.
% %
% We shall differ in two ways:
%
% We allow two disjunctions and two conjunctions (as well as
% two version of units for each).
% %
% We are familiar with the fact that the additive and multiplicative
% version of inference rules are interchangeable when the structural
% rules of weakening and contraction are available.
% %
% Most papers in the theorem proving literature pick just one
% conjunction and one disjunction: the pick is to pick the invertible
% rules.
% %
% Given the goal of FPCs to account for a range of proof evidence, it
% seems better to allow for more connectives and not fewer: for example,
% selecting a left disjunction and discarding the right disjunction can,
% in some situations, be exactly the right inference rule we want for
% disjunction.


The proof certificate continuation for $\forall_c$ is not, in
  fact, a proof certificate but a term-level abstraction of a
  certificate.  When that abstraction is returned from this clerk, it
  is immediately applied to a eigenvariable $y$, thereby forming a
  proof certificate that can incorporate this eigenvariable.


This extension takes place exactly in the place where first-order
terms are manipulated by the kernel: in particular, explicit mapping
between client-side terms (which may contain Skolem functions) and
kernel-side terms (which may contain eigenvariables) must be managed.
%
In all other ways, the FPC framework is left untouched.
%




While Skolemization can soundly be mixed with unification at higher-order
types, certain restrictions must be applied to occurrences of Skolem
functions (so that Skolem functions do not act as choice operators).
%
Furthermore, skolemization can cause an empty type to become
non-empty.
%
Also, the operation of deskolemization can cause many different proofs
to be mapped to just one, suggesting that skolemization introduced
redunancy into the proof process \cite{miller92jsc}.

We note that skolemization, as described above, is not a valid
operation for intuitionistic logic.
%
Implementers of automated theorem provers for intuitionistic logic
have generally employed unification algorithms that are sensitive to
quantifier alternation.
%
It is an easy matter to see that higher-order unification 
is equipped to treat some aspects of quantification alternation.
%
For example, the formulas $\exists x[\lambda y. t = \lambda y. s]$
(with a purely existential quantifier) and $\exists x\forall y[ t =
  s]$ (with a mixed prefix) describe essentially the same unification
problem albeit a unification problem with $\lambda$-terms \cite{miller92jsc}.
%
As a result of this observation, systems such as Isabelle
\cite{paulson94book} and \lP \cite{miller12proghol} treat quantifier
alternation via the unification of $\lambda$-terms.

Since skolemization within formal proofs can be complex and limiting
(only to classical logic with non-empty types), it seems valuable to
see to what extent proof evidence using Skolem terms can be translated
to proof evidence of the original, unskolemized formula.
%


\section{Benefits of removing Skolem functions}

{\color{red} DM: }
        
  While skolemization has advantages for theorem proving (proof
  search), their value for certification and understanding proofs is
  less clear.  In what sense have we exported a proof if it contains
  Skolem terms?  (I.e., we have changed the underlying language).

   1. Foundational.  We need to rely on a minimal set of assumptions.
      FO logic only and no choice principles.

   2. Simple proof checkers.  We only need to implement Gentzen's LK
      system (complete with eigenvariables) but that is an ancient
      system with well understood meaning and good implementations
      (even integrating unification).  For example, it is easy to
      implement the LK proof system in, say, lambda Prolog.  The
      interaction between eigenvariable constraints and unification is
      well-known \cite{miller92jsc} and frequently implemented (in
      systems such as Isabelle, \lP, Twelf).

Cuts are allowed and provide flexible accounting of various forms of
Skolemization.  The proofs we get contain cuts.  Other projects on
skolemization and proof show that there can be very large "cut-free
proofs" resulting from deskolemization (Baaz, et al).  Proofs with
cuts should be more manageable and natural.  One can always consider
doing cut-elimination on the resulting specialized set of
cut-formulas.
