% page limit of 15 page (not including references)
% Final draft deadline is 16 April 2018 (AoE).  Thus: 13h on 17 April.


\documentclass[a4paper,USenglish]{lipics-v2018}

% all macros are here
\usepackage{skolem}

\bibliographystyle{plainurl} % the recommended bibstyle

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\title{A proof-theoretic approach to certifying skolemization}
% Mandatory. Use full name; only 1 author per \author macro;
% first two parameters are mandatory, other parameters can be empty.
%\author{Kaustuv Chaudhuri}
\author{Kaustuv Chaudhuri, Matteo Manighetti, and Dale Miller}
       {Inria \& LIX, \'Ecole Polytechnique\\{Palaiseau, France}}
 %      {Kaustuv.Chaudhuri@inria.fr}
       {}
       {}
       {}

%\author{Matteo Manighetti}
%       {Inria \& LIX, \'Ecole Polytechnique\\{Palaiseau, France}}
%       {Matteo.Manighetti@inria.fr}
%       {}
%       {}

%\author{Dale Miller}
%       {Inria \& LIX, \'Ecole Polytechnique\\{Palaiseau, France}}
%       {Dale.Miller@inria.fr}
%       {https://orcid.org/0000-0003-0274-4954}
%       {}

% mandatory. First: Use abbreviated first/middle names. Second (only
% in severe cases): Use first author plus 'et. al.'
\authorrunning{K. Chaudhuri, M. Manighetti, and D. Miller}

% mandatory, please use full first names. LIPIcs license is "CC-BY";
% http://creativecommons.org/licenses/by/3.0/
\Copyright{Kaustuv Chaudhuri, Matteo Manighetti, and Dale Miller}

% mandatory: Please choose ACM 2012 classifications from
% https://www.acm.org/publications/class-2012 or
% https://dl.acm.org/ccs/ccs_flat.cfm . E.g., cite as "General and
% reference $\rightarrow$ General literature" or \ccsdesc[100]{General
% and reference~General literature}.
\subjclass{F.4.1. Mathematical logic: proof theory}

% mandatory
\keywords{certification, skolemization, sequent calculus, focused
  proof system  {\color{red}\textbf{FIXME}}}

% optional, e.g. invited paper
\category{}

% optional, e.g. full version hosted on arXiv, HAL, or other respository/website
\relatedversion{}

% optional, e.g. related research data, source code, ... hosted on a
% repository like zenodo, figshare, GitHub, ...
\supplement{}

% optional, to capture a funding statement, which applies to all
% authors. Please enter author specific funding statements as fifth
% argument of the \author macro.
\funding{}

%optional
%\acknowledgements{I want to thank \dots}

%Editor-only macros:: begin (do not touch as author)
%\EventEditors{John Q. Open and Joan R. Access}
%\EventNoEds{2}
%\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
%\EventShortTitle{CVIT 2016}
%\EventAcronym{CVIT}
%\EventYear{2016}
%\EventDate{December 24--27, 2016}
%\EventLocation{Little Whinging, United Kingdom}
%\EventLogo{}
%\SeriesVolume{42}
%\ArticleNo{23}
%\nolinenumbers %uncomment to disable line numbering
\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo,
              %DOI, ...), e.g. when preparing a pre-final version to be uploaded to
              %arXiv or another public repository

\begin{document}
\maketitle

\begin{abstract}
When presented with a formula to prove, most theorem provers for
classical first-order logic process that formula following several
steps, one of which is commonly called Skolemization.
%
That process eliminates quantifier alternation within formulas by
extending the language of the underlying logic with new Skolem
functions and by instantiating certain quantifiers with terms built
using Skolem functions.
%
In this paper, we address the problem of checking (i.e., certifying)
proof evidence that involves Skolem terms.
%
Our goal is to do such certification without using the mathematical
concepts of model-theoretic semantics (i.e., preservation of
satisfiability) and choice principles (i.e., epsilon terms).
%
Instead, our proof checking kernel is an implementations of Gentzen's
sequent calculus, which directly support quantifier alternation by
using eigenvariables.
%
We shall describe deskolemization as a mapping from client-side terms,
used in proofs generated by theorem provers, into kernel-side terms,
used within our proof checking kernel.
%
This mapping connects skolemized terms to the internal eigenvariable
abstractions assumes that \emph{outer} skolemization has been
used.
%
Many variations and optimizations of skolemization are certified by
allowing formulas to be manipulated into equivalent forms (e.g.,
miniscoping) prior to applying outer skolemization: the resulting
certified proof retains a cut in which such an equivalent formula is
used as a lemma.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Skolemization is a process (of which there are many variants) that
removes \emph{strong quantifiers} by instantiating such quantifiers
with terms generally of the form $(f~x_1\ldots~x_n)$ where $n\ge 0$
and $x_1,\ldots, x_n$ is a list of distinct \emph{weakly quantified
  variables}.%
\footnote{An occurrence of a quantifier in a formula is \emph{strong}
  if a cut-free proof that introduces it uses an eigenvariable to
  instantiate it.  Otherwise, it is a \emph{weak} quantifier instance.}
%
Exactly which list of such variables is used depends
on which form of skolemization is employed but in all cases, the
resulting formula contains no strong quantifiers.
%
Those implementing theorem provers employ this preprocessing step, in
part because it removes quantifier alternation: when only weak
quantifiers exist, standard first-order unification can be used to
discover how all the remaining quantifiers can be instantiated.
%
Cut-free sequent calculus proofs of skolemized formulas do not contain
occurrences of eigenvariables.

The correctness of skolemization in first-order classical logic
is generally justified by referring to the model theory of classical
logic.
%
In particular, the main meta-theorem surrounding skolemization is that
if the skolemized instance of formula $B$ is satisfiable then
the formula $B$ is also satisfiable.
%
Given that this theorem is about satisfiability (and not proof) then
skolemization is often employed in a \emph{refutation} procedure: if
one can demonstrate that the skolemized version of $\neg B$ is
unsatisfiable (since, for example, one can derive an empty clause from
it), then $\neg B$ is unsatisfiable.
%
Employing the model theory of first-order classical logic again, we
know that $B$ is valid and, hence, by completeness we know that $B$
has a proof.
%
A central issue surrounding the use of skolemization is how to
actually \emph{export} the proof (or refutation) of a skolemized
version of $\neg B$ so that we can formally \emph{certify} that $B$ is
a theorem.

%%% KC: removed these subsection breaks since they don't seem
%%% necessary

% \subsection{Certifying by formalizing model theory and its completeness}
%
In this paper we are interested in \emph{certification} in the sense
of having proofs formally checked using computerized proof-checkers.
%
One method to formally certify a proof using skolemization, is to
formally prove the model-theoretic completeness of first-order
classical logic in a formal reasoning system such as Coq or
Isabelle/HOL.
%
In order to complete such a proof, significant aspects of the
foundations of mathematics would be employed, including axioms of
extensionality, infinity, and choice~\cite{church40}.
%
Certifying that $B$ is provable would then require two steps: (1)
formally checking the proof evidence supplied that skolemized version
of $\neg B$ is unsatisfiable (by checking, for example, that a
refutation refutation is syntactically correct) and then (2) applying
the formalized metatheory of classical logic following the outline
offered above.

% \subsection{Certifying using choice principles}
%
A theorem prover containing a choice operator, such as Hilbert's
$\epsilon$-operator and its associated axioms, can potentially provide
a more targeted justification for using Skolem functions since such
functions can be specified using choice operators.
%
Such a use of the $\epsilon$ operator for justifying skolemization has
been used in Isabelle/HOL~\cite{barbosa17cade}.
%
However, this still leaves unsolved the problem of a direct
certification of a skolemized proof in a system with weaker
foundations (without the axiom of choice, say) and therefore lacks
such operators.

% \subsection{Certifying using deskolemization}

Another more elementary and direct approach would be to
\emph{deskolemize} the proof into a proof in, say, Gentzen's sequent
calculus \LK, which is complete for classical first-order logic
without relying on choice operators or axioms.
%
One does not then need any of the powerful proof techniques behind
completeness and choice principles.
%
Instead, one only needs to check that a proposed proof structure does,
indeed, describe a formal sequent calculus proof.

\subsection{Advantages of building sequent calculus proofs}
\label{ssec:advantages}

While both certification using formalized model theory or using choice
operators are sufficient to convince most people that theorems proved
using Skolem terms are, in fact, true (and therefore provable), there
are a number of reasons why it is important to push harder to actually
build proofs without Skolem functions.

For example, skolemization is not, in general, sound for higher-order
logic (without choice)~\cite{miller87sl} and for intuitionistic logic.
%
If it is possible to build sequent style proofs without Skolem
functions, it should be possible to import such proofs directly into
higher-order provers.
%
It might also be possible to judge that the resulting sequent calculus
proof is intuitionistically valid or not, thereby allowing it to be
imported into provers based only on intuitionistic provers (see, for
example,~\cite{fontaine06tacas,stump13fmsd} of proof evidence being
imported into higher-order proof systems).

It is also the case that if we can provide Gentzen-style \LK-proof from
a proof that used skolemized proof evidence, we have a secured a
low-level logic proof of the theorem for which we can have a
high-degree of confidence.
%
Such a proof should be importable into a wider range of provers, in
particular, provers that do not assume choice principles.

We can imagine future work that involves interacting with, browsing,
and mining \cite{kohlenbach03apal} formal proof structures.  If that
proof relies on just, say, Gentzen's \LK, then the resulting
interactions should be rather direct and informative.  If that proof
relies on mathematical results about choice principles preserving
satisfiability with logics that are extended with Skolem functions,
then that interaction is likely to be more obscure.

\subsection{Our approach to deskolemization}
\label{ssec:deskolem}

Deskolemization has been widely studied for classical first-order
logic.
%
On the theoretical side various kinds of deskolemization results have
been obtained for different forms of skolemization.
%
For example, in~\cite{miller83,miller87sl} it was shown that a certain
type of skolemization (called \emph{outer} skolemization in
Section~\ref{sec:skolemization}) can be deskolemized in expansion
proofs without increasing the size of the expansion proof.
%
A different form of skolemization that is more commonly used in
automated theorem provers (called \emph{inner} skolemization in
Section~\ref{sec:skolemization}) was studied in papers such
as~\cite{avigad03tocl} and~\cite{baaz12jsl} where it was shown that
eliminating Skolem functions can result in complex and expensive
growth of proofs.

In this paper we continue the study of checking and certifying proof
evidence that contains Skolem functions by explicitly deskolemizing
proof evidence and building Gentzen's \LK-style sequent calculus
proofs containing eigenvariables.
%
As we shall see, our approach to deskolemization can be viewed as in a
programming language setting in two ways.
%
First, we identify two different \emph{actors} of a proof checker.
%
The \emph{client} is some theorem prover who wants to export checkable
proofs and the \emph{kernel} is a program that is entrusted to check
proofs in a completely trustworthy fashion.
%
In this setting, the kernel is a logic program and eigenvariables are
an abstraction mechanism used by logic programs to hide some of the
structure of terms \cite{miller90abs}.
%
Since it is impossible for a client to directly refer to such
abstractions, the client must make use of various naming mechanisms in
order to refer to those kernel-side abstractions.
%
As we shall see, Skolem terms serve as one of these naming
mechanisms.


\subsection{Summary of our contributions}

This paper makes the following contributions to the problem of
deskolemizing proof evidence.
%
\begin{enumerate}
\item We provide a modular method to deskolemize proof evidence
  involving Skolem functions into the construction of a sequent calculus
  proof in classical first-order logic.
  %
  Modularity is explicitly provided by the structure of the kernel used
  in the Foundational Proof Certificate (FPC) framework for defining
  proof structures~\cite{chihani17jar}.
  %
  This proof checking framework builds Gentzen-style \LK sequent calculus
  proofs using eigenvariables: such sequent proofs are essentially
  performed and are not generally stored.
  %
  For example, outer skolemization proof evidence (without the use of
  cuts) leads to cut-free and Skolem-free \LK proofs.

\item Prior to performing skolemization, provers often move quantifiers
  within a formula (e.g, anti-prenexing) in order to reduce the number
  of arguments need when building Skolem terms.
  %
  By shortening the list of arguments to Skolem functions, theorem
  provers can expect to find shorter proofs.
  %
  We provide a simple mechanism that allows the checking of proofs that
  employ such optimizations: such optimizations are encoded using a cut
  (i.e., a lemma).

\item We provide a trustworthy implementation of this form of modular
  deskolemization using the higher-order logic programming language
  \lP.
  %
  Simple inspection of our kernel provides rather immediate confidence
  that every successful rule of our proof checker only certifies
  formulas that are, in fact, theorems.
  %
  One must also trust (in our case) the implementation of \lP.
  %
  However, since we are only using the backtracking and higher-order
  unification features of the logic underlying \lP, anyone can provide a
  reimplementation of these features: in this way, one does not need to
  trust the particular implementations of \lP we have used
  (Teyjus~\cite{nadathur99cade} and Elpi~\cite{dunchev15lpar}).
\end{enumerate}

\noindent%
\texttt{\color{red}KC patrolled here on 2018-04-16 15:56:34+0200}

\section{Skolemization}
\label{sec:skolemization}

As is customary, we shall assume that all formulas are in
\emph{negation formal form}: that is, negations have only atomic scope
and the only logical connectives are $\land$, $\lor$, $\top$, $\bot$,
$\forall$, and $\exists$.
%
This normal form is a mild one to assume since the size of a formula
and its negation normal form are essentially the same.
%
We shall also assume that no two occurrences of a quantifier (either
$\forall$ or $\exists$) bind variables with the same name.
%
Alphabetic change of bound variables always make this possible.

Since we are focused on checking proofs, we shall describe
skolemization as a process for replacing universally quantified
formulas with Skolem terms.
%
Formally, replacing universal quantifiers in this way is often called
\emph{herbrandization} while replacing existential quantifiers usually
called  \emph{skolemization}.
%
Since the intent of both operations is to ensure that strong
quantifiers are removed and that eigenvariables are not used within
proofs, it seems unnecessary to introduce a second term and remain
with the more commonly used term skolemization.

Function symbols come equip with arity a collection of function
symbols with their arity is called a \emph{signature}.
%
An example of a signature is $\{a/0, f/1, g/2\}$.
%
We also assume that the set of terms generated from a signature is
non-empty (for example, the collection $\{f/1, g/2\}$ is not a
signature) and that a symbol is given at most one arity within a
signature.

\newcommand{\sksig}{\Sigma_{sk}}

We shall assume that all first-order formulas for which we perform
proof checking contain function symbols and constants from the fix
signature $\Sigma_0$.
%
In order to account for skolemization, we introduce
another signature, $\sksig$, whose members are called \emph{Skolem
  functions}, and which is such that for every arity $n\ge0$, there
are a countably infinite number of members of $\sksig$ of that arity.
%

The following definition, which we take from
\cite{nonnengart01handbook}, seems to be standard.
%
An \emph{outer skolemization step} is a pair of formulas in which
\begin{enumerate}

\item the first formula, say, $B$ is such that if contains the
  subformula $\forall x.C(x)$ that is not in the scope of any
  universal quantifier and which is in the scope of existential
  quantifiers binding the variables $x_1,\ldots,x_n$ ($n\ge0$).

\item the second formula results from replacing that $\forall x.C(x)$
  occurrence in $B$ with $C(f(x_1,\ldots,x_n))$ where $f$ is an
  $n$-arity symbol from $\sksig$ that does not appear in $B$.
\end{enumerate}

An \emph{inner skolemization step} is a pair of formulas that is
defined analogously with the only difference being that the
Skolem term used to instantiate $C(x)$ is $f(y_1,\ldots,y_m)$ where
$y_1,\ldots,y_m$ are the free variables of the occurrence of $\forall
x.C(x)$.  Notice that necessarily, $m\le n$ and that all the variables
in the list $y_1,\ldots, y_m$ are contained in the list $x_1,\ldots,
x_n$.

The formula $E$ is the result of performing \emph{outer
  skolemization} on $B$ if there is a sequent of \emph{outer
  skolemization step} that carries $B$ to $E$ and where $E$ does not
contain any strong quantifiers (i.e., universal quantifiers).
%
Similarly, the formula $E$ is the result of performing \emph{inner
  skolemization} on $B$ if there is a sequent of \emph{outer
  skolemization step} that carries $B$ to $E$ and where $E$ does not
contain any strong quantifiers (i.e., universal quantifiers).

The main result about skolemization is the following theorem.  Its
proof can be found in a number of textbooks and papers: see, in
particular, \cite{andrews81jacm} and \cite[Section
  4.5]{shoenfield67book}.

\begin{theorem}
Let $B$ be a first-order formula over the signature $\Sigma_0$ and
let $E$ is either an inner or outer skolemization of $B$.
%
If $\neg B$ is satisfiable then $\neg E$ is satisfiable.
\end{theorem}

\section{Focused Sequent calculus}
\label{sec:seq}

\long\def\lkfruleshere{
\begin{figure}[tp]
  \emph{Asynchronous rules}
  \begin{gather*}
    \linfer{
      \seq{\Si |- \G \UP A \AND- B, \Th}
    }{
      \seq{\Si |- \G \UP A, \Th} &
      \seq{\Si |- \G \UP B, \Th}
    }
    \quad
    \linfer{
      \seq{\Si |- \G \UP \TOP-, \Th}
    }{}
    \quad
    \linfer{
      \seq{\Si |- \G \UP A \OR- B, \Th}
    }{
      \seq{\Si |- \G \UP A, B, \Th}
    }
    \quad
    \linfer{
      \seq{\Si |- \G \UP \BOT-, \Th}
    }{
      \seq{\Si |- \G \UP \Th}
    }
    \quad
    \linfer[$y \notin \Si$]{
      \seq{\Si |- \G \UP \ALL x. A, \Th}
    }{
      \seq{\Si, y |- \G \UP [y/x] A, \Th}
    }
  \end{gather*}
  \emph{Synchronous rules}
  \begin{gather*}
    \linfer{
      \seq{\Si |- \G \DN A \AND+ B}
    }{
      \seq{\Si |- \G \DN A} &
      \seq{\Si |- \G \DN B}
    }
    \quad
    \linfer{
      \seq{\Si |- \G \DN \TOP+}
    }{}
    \quad
    \linfer{
      \seq{\Si |- \G \DN A \OR+ B}
    }{
      \seq{\Si |- \G \DN A}
    }
    \quad
    \linfer{
      \seq{\Si |- \G \DN A \OR+ B}
    }{
      \seq{\Si |- \G \DN B}
    }
    \quad
    \linfer{
      \seq{\Si |- \G \DN \EX x. A}
    }{
      \seq{\Si |- \WF t }
      &
      \seq{\Si |- \G \DN [t/x] A}
    }
  \end{gather*}
  \emph{Identity rules}
  \begin{gather*}
    \linfer[\text{init}]{
      \seq{\Si |- \G, \NEG p \DN p}
    }{}
    \qquad
    \linfer[\text{cut}]{
      \seq{\Si |- \G \UP \emp}
    }{
      \seq{\Si |- \G \UP A}
      &
      \seq{\Si |- \G \UP \dual A}
    }
  \end{gather*}
  \emph{Structural rules}
  \begin{gather*}
    \linfer[\text{decide}]{
      \seq{\Si |- \G, P \UP \emp}
    }{
      \seq{\Si |- \G, P \DN P}
    }
    \qquad
    \linfer[\text{store}]{
      \seq{\Si |- \G \UP R, \Th}
    }{
      \seq{\Si |- \G, R \UP \Th}
    }
    \qquad
    \linfer[\text{release}]{
      \seq{\Si |- \G \DN N}
    }{
      \seq{\Si |- \G \UP N}
    }
    \\
    {\scriptstyle\text{In the store rule, $R$ is a positive formula or a literal}}
  \end{gather*}
  \caption{Rules of \LKF. $\G$ is a multiset of positive formulas or
    literals, and $\Th$ is a list of formulas.}
  \label{fig:lkf-rules}
\end{figure}}


In Section~\ref{ssec:advantages}, we argued that building explicit
sequent calculus proofs can benefit the certification process.
%
Although we wish to build (or at least perform) a sequent calculus
proof in the sense of Gentzen \cite{gentzen35}, the construction of
such proofs can be highly chaotic.
%
The certification process can be viewed as a kind of protocol between
two agents.
%
One agent is client whose has already found proof evidence, say, a
resolution refutation or an expansion proof.
%
The other agent is the kernel which contains a highly trusted
implementation of, say, Gentzen's \LK sequent calculus proof system.
%
It is the desire of the client to send instructions to the kernel in
order to guide the kernel to build a complete sequent proof.
%
Note that the kernel does not have to build and store the resulting
sequent calculus proof: it will be enough that the kernel preforms
it.

Given this description of the certification process, we can see that
directly employing the original sequent calculus could be highly
problematic.
%
Attempting to build a proof of a sequent can, in principle, depend a
great deal of communication to go between the client and the kernel
since nearly every sequent can be the conclusion of a structure rules
(weakening and contraction), a cut rule, and a (possibly large)
number of inference rules.
%
And once the client instructs the kernel to attempt one such inference
rule, it is likely that one or two new sequents (the premises of the
applied rule) need proofs and that each of these requires again
essentially the same kind of instructions.
%
Clearly, such a simplistic kernel, with its demand to organized it
``micro-rules'',  puts an enormous burden on the client.

Fortunately, recent advances in the sequent calculus, namely, the
discovery of polarization and focusing---first developed for linear
logic \cite{andreoli92jlc,girard91mscs} and then extended to classical
and intuitionistic logic---have made it possible to design highly
structured and greatly reduced protocols between such clients and
kernels.
%
For example, as we shall see, logical connectives with \emph{negative}
polarity have invertible introduction rules: those when these
connectives appear in a sequent, no communication needs to take place
between the client and kernel since the kernel can simply eagerly
apply the inference rules associated to invertible connectives without
loss of provability.
%
Similarly, connectives with \emph{positive} connectives are selected
as a \emph{focus} of the kernel's proof building process: the kernel
may need to ask the client to suggest non-invertible rules to apply in
this case, but as long as the focus remains, the kernel can be
structured to only request help with that one formula (and not the
many other formulas surrounding it in the sequent).
%
In this section, we details of a focused proof system for
first-order classical logic and in the next section, we describe how
to formally exploit such highly structured sequent calculus proofs to
yield the protocol between client and kernel that is the basis of the
\emph{foundational proof certificate} framework.

The basis of our certification is a variant of the \LKF proof
system~\cite{liang09tcs}, which is a sequent calculus for classical
first-order logic given in the Gentzen-Sch\"utte style (a.k.a. Tait
style), based on the system GS[1, 2, 3]~\cite{troelstra00book}.
%
\emph{Terms} ($s, t, \dotsc$) will, as usual, be built from variables
($x, y, \dotsc$) and \emph{function applications} of the form
$f(t_1, \dotsc, t_n)$ where $f$ is a \emph{function symbol} of arity
$n$.
%
Formulas ($A, B, \dotsc$) will belong to the following grammar, which
we divide into the two \emph{polarities}, \emph{positive}
($P, Q, \dotsc$) and \emph{negative} ($N, M, \dotsc$), that we explain
below.
%
\begin{align*}
  A, B, \dotsc &::= {
    P \mid N
  } \tag*{(formulas)} \\
  P, Q, \dotsc &::= {
    p \mid A \AND+ B \mid \TOP+ \mid A \OR+ B \mid \BOT+ \mid
    \EX x. A
  } \tag*{(positive formulas)} \\
  N, M, \dotsc &::= {
    \NEG p \mid A \AND- B \mid \TOP- \mid A \OR- B \mid \BOT- \mid
    \ALL x. A
  } \tag*{(positive formulas)} \\
  L &::= {
    p \mid \NEG p
  } \tag*{(literals)}
\end{align*}
%
Here, $p$ ranges over positive \emph{atomic formulas} that are always
of the form $a(t_1, \dotsc, t_n)$ where $a$ is some predicate symbol
of arity $n$.
%
We write $\dual A$ for the de Morgan dual of $A$, given by the pairs
$p/\NEG p$, ${\AND+}/{\OR-}$, ${\TOP+}/{\BOT-}$, ${\OR+}/{\AND-}$,
${\BOT+}/{\TOP-}$, and ${\exists}/{\forall}$.

\lkfruleshere

For the non-quantifier connectives, the polarity amounts to an
annotation on the formula; the quantifiers, on the other hand, have a
unique polarity.
%
The polarity annotations do not affect the truth of a formula, so
$A \AND+ B$ and $A \AND- B$ are equivalent.
%
However, positive and negative formulas have very different proofs.
%
% The intuitive meaning of invertible is central here.  We should say
% instead that invertible is put into asynchronous (whose informal
% meaning is ``can be done without communications with others).
%
% Intuitively, a negative formula behaves \emph{asynchronously}, which
% is to say that all its introduction rules have the property that the
% collection of premises are \emph{equivalent} to the conclusion.
%
Intuitively, the introduction rules for negative formula are
\emph{invertible}: that is, these rules have the property that their
collection of premises are \emph{equivalent} to the conclusion.
%
These invertible inference rules are organized into the
\emph{asynchronous phase}: that is, a grouping of inference rules for
which the kernel can apply without needing to communicate with the
client.
%
For instance, the rules for $\AND-$ and $\OR-$ are the following (modulo
certain minor differences explained below):
%
\begin{gather*}
  \infer{
    \seq{|- A \AND- B, \D}
  }{
    \seq{|- A, \D} & \seq{|- B, \D}
  }
  \qquad
  \infer{
    \seq{|- A \OR- B, \D}
  }{
    \seq{|- A, B, \D}
  }
\end{gather*}
%
A positive (non-atomic) formula, on the other hand, has an inference
rules that is not necessarily invertible, meaning that its
introduction rule may involve a choice and its premise(s) may not be
equivalent to its conclusion.
%
As a result, such inference rules are organized into the
\emph{synchronous phase}: that is, a grouping of inference rules for
which the kernel needs to communicate with the client.
%
For $\OR+$, for instance, the synchronous rules are:
%
\begin{gather*}
  \infer{
    \seq{|- A \OR+ B, \D}
  }{
    \seq{|- A, \D}
  }
  \qquad
  \infer{
    \seq{|- A \OR+ B, \D}
  }{
    \seq{|- B, \D}
  }
\end{gather*}
%
These rules encode an essential choice between the two operands $A$
and $B$.
%
The benefit of having both polarized variants of $\OR$ is that our
framework will be able to build more proofs in a more flexible fashion.
%
% the full range of proof behaviors is available. {\color{red} Better
% explanation of the power of polarities}

Following a technique pioneered by Andreoli~\cite{andreoli92jlc}, we
separate the two kinds of inference rules by means of two kinds of
sequents:

\smallskip
\begin{tabular}{l@{\qquad}l}
  $\seq{\Si |- \G \DN A}$ & synchronous sequent with $A$ \emph{under focus} \\
  $\seq{\Si |- \G \UP \Th}$ & asynchronous sequent
\end{tabular}
\smallskip

\noindent%
where the \emph{context} $\G$, called the \emph{store}, is a multiset
of positive formulas or literals, and $\Th$, called the
\emph{asynchronous zone}, is a \emph{list} of formulas.
%
$\Si$ is the \emph{signature}, which is a set of \emph{eigenvariables}
that can be free in the terms to the right of $\seq{|-}$.
%
An asynchronous sequent of the form $\seq{\Si |- \G \UP \emp}$ is
called a \emph{neutral sequent}.

The full list of inference rules for \LKF is in Figure~\ref{fig:lkf-rules}.
%
A proof in \LKF can be seen as an alternation of two kinds of
\emph{phases}, reading the rules from conclusion to premises.
%
The \emph{synchronous phase} starts with a neutral sequent as
conclusion; a positive formula is chosen for \emph{focus} and in the
entire phase the focused formula is required to be principal.
%
The synchronous phase may close the proof with the init rule when the
focused formula is an atom, or may transition to the
\emph{asynchronous phase} with the release rule that is applicable
when the focus is a negative formula.
%
(Note that in the init rule if the dual of the focused formula is not
in the context then the proof attempt is considered a \emph{failure}
since there is no other inference rule available to prove a focus
on a positive literal.)
%
In the asynchronous phase a rule is applied to the leftmost formula in
the asynchronous zone; if it is a positive formula or a literal, it is
stored, and in every other case an asynchronous rule is used to
decompose this formula.
%
Finally, when the asynchronous zone is empty, i.e., when we are back
to a neutral sequent, then the cycle begins anew.

Let $B$ be an unpolarized formula and let $\hat B$ be a polarized
formula that results from placing either a $+$ or $-$ superscript on
every propositional formula.
%
We shall also assume that atomic formulas are polarized arbitrarily:
they could be all negative, all positive, or some mixture of these
two.
%
The following theorem is proved in~\cite{liang09tcs}.

\begin{theorem}[Soundness and Completeness of \LKF]
  Let $B$ be a formula of first-order classical logic.  If $B$ is a
  theorem, then $\seq{\emp |- \emp \UP \hat B}$ is derivable for every
  polarized version $\hat B$ of $B$.  Furthermore, if $\seq{\emp |-
    \emp \UP \hat B}$ is provable for some polarized version $\hat B$
  of $B$, then $B$ is a theorem.
\end{theorem}

Note that polarization does not affect provability but it can and does
have significant impact on the size and shape of proofs.

% \noindent%
%
% {\color{red}Some stuff to say here about how this \LKF system is to be
%   seen as the ultimate ``blessed'' proof system.
%
%  DM I don't think we need to claim it is ultimate or blessed.  It's
%  just an example of a focused system and the reason for having them
%  is now motivated at the start of this section.}

\section{Augmented \LKF and Foundational Proof Certificates}
\label{sec:fpc}

\long\def\lkfaruleshere{
\begin{figure}[t]
  \emph{Asynchronous rules}
  \begin{gather*}
    \linfer{
      \seq{\Xi_0; \Si |- \G \UP A \AND- B, \Th}
    }{
      \seq{\Xi_1; \Si |- \G \UP A, \Th} &
      \seq{\Xi_2; \Si |- \G \UP B, \Th} &
      {\AND_c}(\Xi_0, \Xi_1, \Xi_2)
    }
    \qquad
    \linfer{
      \seq{\Xi_0; \Si |- \G \UP \TOP-, \Th}
    }{
%      {\color{red} {\TOP_c}(\Xi_0) ???} We could have put a clerk
%      here or not.  The currently system that we have published does
%      not have such a clerk.
    }
    \\[1ex]
    \linfer{
      \seq{\Xi_0; \Si |- \G \UP A \OR- B, \Th}
    }{
      \seq{\Xi_1; \Si |- \G \UP A, B, \Th} &
      {\OR_c}(\Xi_0, \Xi_1)
    }
    \qquad
    \linfer{
      \seq{\Xi_0; \Si |- \G \UP \BOT-, \Th}
    }{
      \seq{\Xi_1; \Si |- \G \UP \Th} &
      {\BOT_c}(\Xi_0, \Xi_1)
    }
    \\[1ex]
    \linfer[$y \notin \Si$]{
      \seq{\Xi_0; \Si |- \G \UP \ALL x. A, \Th}
    }{
      \seq{\Xi_1 ; \Si, \CP t y |- \G \UP [y/x] A, \Th} &
      {\forall_c}(\Xi_0, \Xi_1, t)
    }
  \end{gather*}
  \emph{Synchronous rules}
  \begin{gather*}
    \linfer{
      \seq{\Xi_0; \Si |- \G \DN A \AND+ B}
    }{
      \seq{\Xi_1; \Si |- \G \DN A} &
      \seq{\Xi_2; \Si |- \G \DN B} &
      {\AND_e}(\Xi_0, \Xi_1, \Xi_2)
    }
    \quad
    \linfer{
      \seq{\Xi_0; \Si |- \G \DN \TOP+}
    }{
      {\TOP\mkern -3mu_e}(\Xi_0)
    }
    \\[1ex]
    \linfer[$i \in \set{1, 2}$]{
      \seq{\Xi_0; \Si |- \G \DN A_1 \OR+ A_2}
    }{
      \seq{\Xi_1; \Si |- \G \DN A_i} &
      {\OR_e}(\Xi_0, \Xi_1, i)
    }
    \qquad
    \linfer{
      \seq{\Xi_0; \Si |- \G \DN \EX x. A}
    }{
      \seq{\Si |- \CP t s } &
      \seq{\Xi_1; \Si |- \G \DN [s/x] A} &
      {\exists_e}(\Xi_0, \Xi_1, t)
    }
  \end{gather*}
  \emph{Identity rules}
  \begin{gather*}
    \linfer[\text{init}]{
      \seq{\Xi_0; \Si |- \G, l{:}\NEG p \DN p}
    }{
      \text{init}_e(\Xi_0, l)
    }
    \qquad
    \linfer[\text{cut}]{
      \seq{\Xi_0; \Si |- \G \UP \emp}
    }{
      \seq{\Xi_1; \Si |- \G \UP A} &
      \seq{\Xi_2; \Si |- \G \UP \dual A} &
      \text{cut}_e(\Xi_0, \Xi_1, \Xi_2, A)
    }
  \end{gather*}
  \emph{Structural rules}
  \begin{gather*}
    \linfer[\text{decide}]{
      \seq{\Xi_0; \Si |- \G, l{:}P \UP \emp}
    }{
      \seq{\Xi_1; \Si |- \G, l{:}P \DN P} &
      \text{decide}_e(\Xi_0, \Xi_1, l)
    }
    \qquad
    \linfer[\text{release}]{
      \seq{\Xi_0; \Si |- \G \DN N}
    }{
      \seq{\Xi_1; \Si |- \G \UP N} &
      \text{release}_e(\Xi_0, \Xi_1)
    }
    \\[1ex]
    \linfer[\text{store}]{
      \seq{\Xi_0; \Si |- \G \UP R, \Th}
    }{
      \seq{\Xi_1; \Si |- \G, l{:}R \UP \Th} &
      \text{store}_c(\Xi_0, \Xi_1, l)
    }
    \\
    {\scriptstyle\text{In the store rule, $R$ is a positive formula or a literal}}
  \end{gather*}
  \caption{Rules of \LKFa, an augmented version of \LKF. $\G$ is a
    multiset of pairs of the form $l{:}R$ where $l$ is an index and
    $R$ is a positive formulas or literals, and $\Th$ is a list of
    formulas.}
  \label{fig:lkfa-rules}
\end{figure}}

In this section we will describe how we use the \LKF system to build a
protocol for mediating the communications between a client, who
already some proof evidence in hand, and the kernel, (a.k.a. the proof
checker).
%
This protocol is the basis for the \emph{foundational
  proof certificates} framework~\cite{chihani17jar}.
%
The key idea is to augment the \LKF proof system as follows.
%
\begin{itemize}
\item A \emph{proof certificate} is threaded througth every sequent
  and inference rule: these certificates are term structures that
  contain the clients proof evidence.
\item Additional premises are added to the \LKF inference rules: these
  premises manipulate and extract information from
  proof certificates and serve as guards or handlers for inference
  rules.
\end{itemize}
%
There are two kinds of additional premises added to inference rules.
%
The first kind, the \emph{clerks}, are added to asynchronous rules:
clerks perform routine maintance of proof certificate information.
%
% For instance, in the case of $\forall$-introduction, the clerk would
% use the eigenvariable that is generated by the proof checker to
% suitably modify the proof certificate of the premise of the rule.
%
The second kind, the \emph{experts}, are added to synchronous rules
and they are responsible for attempting to find important information
within the proof certificate to guide the possible choices of the
kernel.
%
For instance an expert may inform the kernel which of the two rules to
use for $\OR+$-introduction or which witness term to use for
$\exists$-introduction.

\lkfaruleshere

The augmented version of \LKF will be called \LKFa, uses the
following kinds of sequents.

\smallskip
\begin{tabular}{l@{\qquad}l}
  $\seq{\Xi ; \Si |- \G \DN A}$ & synchronous sequent with $A$ \emph{under focus} \\
  $\seq{\Xi ; \Si |- \G \UP \Th}$ & asynchronous sequent
\end{tabular}
\smallskip

\noindent%
Both of the structures $\Si$ and $\G$ are generalized in the \LKFa
over what they were in \LKF.
%
In particular, $\Si$ is now more than a signature: is a set of pairing
of the form $\CP t y$ where $t$ is a client-side term (containing,
for example, Skolem functions) is associated to the eigenvariable $y$
(that is, a kernel-side term).
%
In a similar fashion, the context $\G$ is extended to be a set of pair
of the form $l{:}R$ where $l$ is an \emph{index} and $R$ is a positive
formula or a literal.
%
The exact structures behind indexes is not specified by the kernel but
is a detail provided by the definition of a proof certificate format.
%
The context $\Th$ is as before in \LKF.

There are several important things to observe about the \LKFa calculus
shown in Figure~\ref{fig:lkfa-rules}.
%
First, predicates with subscript $_e$ are experts and those with
subscript $_c$ are clerks.  We drop the explicit reference to the
polarity of clerks and experts since these can be inferred easily:
e.g., we write ${\AND}_c$ instead of ${\AND-}_c$ since clerks are
defined for negative connectives.
%
Second, the first argument to the expert or clerk is always the proof
certificate of the conclusion, and can be interpreted as an input. The
other proof certificate arguments can be interpreted as outputs
yielding the continuation proof certificates for the premises (if
any). There are also additional arguments that may be indexes (in the
case of $\text{init}_e$, $\text{decide}_e$, and $\text{store}_c$),
client-side name to associate with an eigenvariable (in the case of
$\forall_c$), rule selectors (in the case of ${\OR}_e$), witness terms
(in the case of $\exists_e$), or formulas (in the case of
$\text{cut}_e$).

The predicate $\CP \cdot \cdot$ in the \LKFa proof system can be
formally defined using \emph{copy-clauses}, a standard technique used
to encode both term-level equality and substitutions in logic
programming \cite{miller91iclp}.
%
The copy-clauses based on the signature $\{a/0, f/1, g/2\}$ have the
following \lP specification.
\begin{lstlisting}[language=prolog]
copy a a.
copy (f X)   (f U)   :- copy X U.
copy (g X Y) (g U V) :- copy X U, copy Y V.
\end{lstlisting}
It is easy to show that if $t$ and $s$ be two closed terms over the
signature $\{a/0, f/1, g/2\}$, \lsti{copy t s} is provable from these
clauses if and only if $t=s$.
%
Obviously, an arbitrary first-order signature can be translated into
such a set of copy-clauses. 

The inference rules in Figure~\ref{fig:lkfa-rules} can be implemented
directly in \lP, as has been described in
several other papers \cite{blanco17cade,chihani13pxtp,chihani17jar}.
%
Although such implemementations can be small, we present here only a
few clauses.  First, two simple clauses.
\begin{lstlisting}[language=prolog]
async Cert [(A or- B)|R] :- orC Cert Cert', async Cert' [A, B|R].
sync  Cert (A or+ B) :- orE Cert Cert' C, ((C = left,  sync Cert' A);
                                           (C = right, sync Cert' B)).
\end{lstlisting}
Here, the proof theory judgments $\seq{\Xi; \Si |- \G \UP \Th}$ and
$\seq{\Xi; \Si |- \G \DN A}$ are represented by the atomic formulas
\lsti{(async Cert Theta)} and \lsti{(sync Cert A)}, respectively: the
encoding of $\Si$ and $\G$ are captured by features found in the
(intuitionistic) logic underlying \lP.
%
Thus, the two clauses above implement the intended meaning the of
focused introduction rules for $\OR-$ and $\OR+$, respectively.

The introduction rules for the quantifiers employ the copy-clauses to
translate client-side terms to kernel-side terms.
%
In particular, consider the following two \lP clauses specifying the
introduction of the quantifiers.%
\footnote{The explicit translation based on copy-clauses was not a
  feature of earlier FPC kernels
  \cite{blanco17cade,chihani13pxtp,chihani17jar}: in those earlier paper,
  substitution terms were either not stored in proof certificates or
  there were no difference between client-side and kernel-side terms
  since theorem did not contain strong quantifiers.
{\color{red} Maybe this should be said more plainly since otherwise
  reviewers might think that this section has nothing new in it.}}
%
\begin{lstlisting}[language=prolog]
async Cert [all B|R] :- allCx Cert Cert' T,
                        pi w\ copy T w => async (Cert' w) [B w|Rest].
sync Cert (some B) :- someE Cert Cert' T, copy T S, sync Cert' (B S).
\end{lstlisting}
%
Note that the universal implication of \lP (\lsti{pi w\}) implements
  the eigenvariable feature needed for the \LKFa proof system and that
  the implication \lsti{=>} is used to assume the atomic fact
  \lsti{(copy T w)}.
In this way, the $\Sigma$ context in Figure~\ref{fig:lkfa-rules} is
implemented via \lP's intuitionistic context.

The copy-clauses can now be used uniformly to perform
\emph{deskolemization} in the following sense.
%
Assume that both the kernel and client both agree on the signature
$\Sigma_0$ and that the copy-clauses derived from that signature, say,
$\Cscr{\Sigma_0}$. 
%
As proof checking progresses, new copy-atomic formulas are added to
the $\Sigma$ context whenever a strong quantifier is encounter (via
the first clause displayed above).
%
Whenever the client computes (via the existential expert \lsti{someE})
a client-side term \lsti{T} is then translated to the kernel-side
formula \lsti{S} by the query \lsti{copy T S}.

\begin{example}
Assume that the base signature for both the client and the kernel is
$\{a/0, f/1, g/2\}$.  Also assume that the client is using $h/1$ as a
Skolem function and that the kernel has introduced two eigenvariables
$x$ and $y$ and that $\G$ contains the associations 
\lsti{(copy (h a) x)} and \lsti{(copy (h (f a)) y)}.  Then the \lP
query \lsti{(copy (g (h (f a)) (f (h a))) X)}, for some logic variable
$X$, will have a unique solution, namely, the one that binds \lsti{X}
to \lsti{(g y (f x))}.  It is this step that performs
deskolemization. 
%
Note, however, that we do not necessarily assume that deskolemization
is determiniate.  
%
In particular, if the $\G$ context contained the atoms 
\lsti{(copy (h a) x)} and \lsti{(copy (h a) y)}, then there are two
solutions to the query \lsti{(copy (g (h a) (f a)) X)}, namely,
binding \lsti{X} to either \lsti{(g x (f a))} or \lsti{(g y (f a))}.
%
Nondeterminism in deskolemization is not a soundness problem in the
context of the kernel we have described here: instead, this
nondeterminism may cause the kernel to backtrack and to example more
than one deskolemization in order to finish proof checking.
\end{example}

Observe that given an \LKFa sequent, we can easily obtain a
corresponding \LKF sequent by removing the proof certificate and the
indexes on the formulas in the store; call this its \emph{underlying
  sequent}.
%
The following property is obvious.

\begin{theorem}[Soundness of \LKFa]
  If an \LKFa sequent is derivable, then its underlying sequent is
  derivable in \LKF and the unpolarized version of that sequent is \LK
  provable.
\end{theorem}

\noindent%
The completeness of \LKFa
depends on the specification of the clerk and expert predicates
supplied by the client.
%
It is important to note that \LKFa is sound by construction: no
  specification for the clerks and experts provided by the
  client can lead the kernel to prove a non-theorem.
%
This property is a critical feature of a proof checking kernel.

{\color{red}DM Still to consider to some extent:
  \begin{itemize}
  \item Describe FPCs
  \item Talk about client vs. kernel views (indexes, polarities)
   \item We still need to be clear that polarization maps formulas and
     skolemization maps terms from client to kernel.
     \item There is also the parallel between skolem-terms-as-names
       and indexes-as-surrogate formulas.
  \end{itemize}
}


\section{Various forms of skolemization}
\label{sec:deskolem}

{\color{red}\bf DM will continue here by about 21:00.}

{\color{red}
  Goal here: return to the topic of outer skolemization and inner too.
  What is a general approach to justifying these different approaches?
  Use a cut-formula, etc.
  }

\emph{NOTE (Matteo): I am not sure whether it is actually useful to
have a paragraph dedicated to miniscoping, given that the treatment is
analogous to any other optimization. The question about inner
skolemization still prevents me from having a very clear view of this
section.}

Different kinds of skolemization (e.g., outer vs inner) can greatly
influence the complexity of proof search.
%
For this reason, theorem provers employ various kinds of optimizations
to the standard skolemization in order to have more control on the
term generation.
%
The simplest of these techniques consists in moving, when possible,
quantifiers in or out over other connectives.
%
In some cases, proofs will contain smaller Skolem terms while in other
cases proofs will contain fewer but bigger terms.
%
Optimizations techniques for skolemization can be rather
sophisticated: see, for example, \cite{goubault95jiglp} for a
technique using BDDs that reduces dependencies on weak variables when
performing skolemization.
%
In this section we will see how we can justify proof evidence obtained
with uses of some optimizations.

\subsection{Miniscoping}

Very often automated theorem provers benefit from having Skolem terms
with a lower arity \cite{robinson01book}. The most important
transformation technique to this aim is \emph{Miniscoping}, consisting
in pushing quantifiers as inwards as possible, in order to minimize
the scope of quantifiers. Let's formally define what a miniscoped
formula is.

\begin{definition}[Miniscoping rules, miniscoped formula]
\mbox{}
  \begin{itemize}
  \item   $\forall x \ (\varphi(x,z) \land \psi(x,z)) \mapsto  (\forall x_1 \ \varphi(x_1,z)) \land (\forall x_2 \ \psi(x_2,z))$
  \item   $\exists x \ (\varphi(x,z) \lor \psi(x,z)) \mapsto  (\exists x_1 \ \varphi(x_1,z)) \lor (\exists x_2 \ \psi(x_2,z))$
  \item   $\mathcal{Q} x \ (\varphi(x,z) \circ \psi(z)) \mapsto  (\mathcal{Q} x \ \varphi(x,z)) \circ \psi(z)$
  \item   $\mathcal{Q} x \ (\varphi(z) \circ \psi(x,z)) \mapsto  \varphi (z)  \circ (\mathcal{Q} x \ \psi(x,z))$
  \end{itemize}
Where $\mathcal{Q}$ is any of $\forall, \exists$ and $\circ$ is any of
$\land, \lor$. A formula is said to be \emph{miniscoped} if none of
the miniscoping rules is applicable.
\end{definition}

Miniscoping only involves changing the scope of quantifiers, and
doesn't otherwise change the logical structure of formulas: clearly
the original and miniscoped formulas are logically equivalent. It is
however very well known \cite{baaz94fi} that this can have a dramatic
impact on the size of cut-free deskolemized proofs.

We take a different approach here and allow the cut rule in the
checking procedure. We also require the client to communicate that
miniscoping has been applied prior to skolemization.

Given a formula $F$, it is then easy to compute its miniscoped version
$F'$, and then produce proof evidence that $F' \vdash F$. We can then
check the skolemized proof evidence against the unskolemized $F'$ with
our technique, and a single cut will yield proof evidence for $F$.

\subsection{Other optimizations}

\emph{NOTE (Matteo): De Nivelle \cite{denivelle02csl} is a good
citation for transforming various optimized skolemizations to standard
ones. I didn't include it since it targets inner skolemization, and
our stance on this is unclear.}

The discussion about miniscoping can be generalized to other kind of
optimization. A theorem prover could apply any number of clever
operations to a formula when it believes that it will obtain better
results when applying skolemization.

We require for these cases that the client describes these
operations. In the case of miniscoping, the entailment could be easily
checked; in the case of more clever optimizations, the client will
also need to provide justifications for them.

The optimizations will finally be included in the proof checking
procedure in the form of cuts, as it was the case for miniscoping.

\subsection{The topic of inner skolemization}

Thus, a cut-free proof using outer-skolemization yields a cut-free
proof using inner-skolemization.  The converse is, however, not
necessarily true.

We might be faced with a situation in which we have a cut-free proof
of $sk_i(B)$ (using inner skolemization) but no simple way to
construct a cut-free proof of $B$.  This is a topic (really? check
this) addressed by Baaz and others.  Since inner skolemization is
sound (proved by Andrews?), then the existence of a proof of inner
skolemization of B means that B is valid and, hence, it has a cut-free
proof (by completeness and cut-elimination).  The resulting proof size
can be much larger (again, Baaz et al).

Relate innermost skolemization with miniscoping.  Matteo has a
counterexample to the claim: innermost skolemization is the same as
miniscoping and then using outermost skolemization.

Thus, we might need to resign to using ``miniscoping plus outermost''
as opposed to innermost.  If we live with this limitation, then we can
automatically generate the cut/lemma formula.  (The generation of the
formal proof of entailment with miniscoped formulas is still a bit
tricky...)



\section{Experiments with an implementation}
\label{sec:implementation}
Several experimental implementations were carried out, adapting
existing FPC code and crafting new ones to demonstrate the improved
kernel. They are available on the web at \emph{(ref)}.

Here we will concentrate on describing the implementations concerning
Expansion Trees. Expansion Trees \cite{miller87sl} are a proof
formalism that generalizes the idea of Herbrand disjunctions to
formulas with arbitrary quantifiers.
%
We assume here formulas to be in negation normal form, and polarize
negatively all the connectives. We can define the Expansion Tree of a
formula F in the following way:

\begin{definition}[Expansion Tree] \mbox{}
  \begin{itemize}
  \item If $A$ is an atomic formula, it is an Expansion Tree for
itself
  \item If $Q_1, Q_2$ are Expansion Trees of $F_1,F_2$, then $eOr\,
Q1\, Q2$ and $eAnd\, Q1\, Q2$ are Expansion Trees for $F_1 \lor F_2$
and $F_1 \land F_2$ respectively
  \item If $u$ is a variable (called \emph{select variable}) and $Q$
is an expansion tree of $F$, then $eAll \, u \, Q$ is an Expansion
Tree for $\forall x \, F$
  \item If $t_1,\dots,t_n$ is a list of terms and $Q$ is an Expansion
Tree for $F$, then $eSome \, [(t_1,Q),\dots,(t_n,Q)]$ is an Expansion
Tree for $\exists x \, F$
  \end{itemize}
\end{definition}

Terms in existential nodes can make use of select variables. The
original definition states in addition to this some correctness
criteria: we do not need them in this context, since correctness is
guaranteed by the kernel.

Expansion trees are also central in the deskolemization procedure
described in \cite{baaz12jsl}, of which implementations exist (such as
GAPT, \cite{ebner16ijcar}).
%
Indeed, the notions of select variables and of terms using them puts
Expansion Trees very close to the realm of skolemization: select
variables can be seen as nothing but another mechanism for naming
eigenvariables, in the spirit of client vs. kernel terms.

We implemented three procedures for checking different kinds of proof
evidence based on this formalism: one for Expansion Trees, one for the
slightly different Skolem Expansion Trees, and one for Expansion Trees
of skolemized formulas.

\begin{figure}
  \begin{lstlisting}[language=prolog]
kind et       type.  % expansion trees
kind qet      type.  % quantified trees (leading introduction of select vars)

type idx             form -> index.

typeabbrev subExp    list (pair i et).    % Expansions for a node
typeabbrev context   list (pair form et). % Basic elements of contexts

type eIntro            (i -> qet) -> qet.
type eC                et -> qet.

type eLit,eTrue, eFalse     et.
type eAnd, eOr         et -> et -> et.
type eAll              i  -> et -> et.
type eSome             subExp    -> et.

type astate            context -> context        -> cert.
type sstate            context -> (pair form et) -> cert.
  \end{lstlisting}
  \caption{Certificate constructors for Expansion Trees}
  \label{fig:exp-cert}
\end{figure}

\subsection{Expansion Trees}

We start from describing a basic FPC that can check proof evidence in
the form of an expansion tree.
%
The signature of the FPC, described in figure~\ref{fig:exp-cert},
contains two certificate constructors: \texttt{astate} is consumed
during the asyncronous phase and records two contexts representing the
storage and the asynchronous zone; \texttt{sstate} is consumed during
the synchronous phase and records the storage and the formula under
focus.
%
Formulas are paired in the certificate with the expansion trees to
which they are associated. The only index constructor uses the
formulas themselves as indexes.

\begin{figure}
\begin{lstlisting}[language=prolog]
orC    (astate Left ((pr B (eOr E1 E2))::Qs))
       (astate Left ((pr B1 E1)::(pr B2 E2)::Qs)) :- disj- B1 B2 B.
andC   (astate Left ((pr B (eAnd E1 E2))::Qs))
       (astate Left ((pr B1 E1)::Qs))
       (astate Left ((pr B2 E2)::Qs))             :- conj- B1 B2 B.
allCx  (astate Left ((pr ForallB (eAll Uvar E))::Qs))
       (w\ astate Left ((pr (B w) E)::Qs)) Uvar :- all- B ForallB.
someE  (sstate Left (pr Form (eSome [pr Term ExTree])))
       (astate Left [(pr (Body Term) ExTree)])
       Term :- some+ Body Form.
\end{lstlisting}
  \caption{FPC for expansion trees}
\label{fig:exp-fpc}
\end{figure}

The main clerks and experts are presented in
figure~\ref{fig:exp-fpc}. Since connectives are polarized negatively,
most of the work is carried out by clerks that simply consume the
connectives and trees and add the components to the updated state.

When meeting a strong quantifier, the expansion tree contains the
select variable associated to it: we will then use the new
\texttt{allCx} to instruct the kernel to create a new eigenvariable,
and provide the select variable as client name for that eigenvariable.

When we meet an existential node, together with the list of terms by
which the existential should be instantiated, we can simply
communicate the client term \emph{T} to the kernel, that will proceed
to translate it to a kernel term. Note that in the code we made the
assumption that only one term is present in the list: this is due to
how contratction is treated, and is outside of the current scope.

\subsection{Skolem Expansion Trees}
Skolem Expansion Trees are a structure introduced in the usual process
of deskolemization through expansion trees. The only difference with
usual expansion trees is that universal nodes are not instantiated by
select variables, but by Skolem terms.
%
Thus we can see them as a kind of skolemized proof evidence, where the
client preserves the information of the association between Skolem
terms and eigenvariables.

Given the discussion on the FPC for Expansion Tree, it is clear that
this new setting does not provide any challenge to the old FPC: it is
just presented with slightly different looking client terms, and will
work exactly in the same way.

\subsection{Expansion Trees of Skolemized formulas}
We now turn our attention to a fully skolemized setting, where we are
given an expansion tree relative to a skolemized formula and we want
to check it against the original one.
%
We only need to apply to the FPC the modification that was introduced
in Section~\ref{sec:desk-outerm-skol}. First, we note that since there
are no strong quantifiers left in the skolemized formula, the
Expansion Tree will not contain any select variable node.
%
Accordingly, we modify the \textsc{allCx} clerk in the following way

\begin{lstlisting}[language=prolog]
allCx  (astate Left ((pr ForallB E)::Qs))
       (w\ astate Left ((pr (B w) E)::Qs)) Sk :- all- B ForallB.
\end{lstlisting}

Thus, when the checker finds a strong quantifier it will be instructed
to create a new eigenvariable, and it will use a logic variable as the
name for it. This variable will ultimately be unified with the correct
Skolem term.


\section{Related and future work}
\label{sec:related}

Summarizing, we have proposed an extension to the framework of
Foundational Proof Certificates, that allows us to modularly extend
definitions for various kinds of proof evidence in order to be able to
check skolemized proofs. We have described the implementation of the
improved kernel, and discussed some implemented examples.

% Interesting observation: instantiating Skolem functions preserves
% proof checking and reconstruction but increases nondeterminism in
% checking.

% Skolemization has limited applications within intuitionistic logic
% (see Dowek and Werner). -- DM Note so important to cite here.  It
% seems to never have been published.

Future lines of work include extending this to the higher-order
setting. Skolemization work similarly at higher-order quantification,
and we expect our approach to naturally extend to this case.
%
There have been several different approaches to deskolemization in the
past.  Ours stands in contrast to the paper \cite{reger17arcade} by
Reger and Suda, where certificates are allowed to involve inference
rules that preserve satisfaction: this was proposed there to treat,
for example, Skolemization.  We shall not consider such extensions to
proof certificates here.
%
F\"arber and Kaliszyk \cite{farber16paar} proceed in a similar way as
we did, but obtain less general result as the work is directly linked
to the formalisms of Resolution and Natural Deduction.
%
De Nivelle\cite{denivelle05ic} proceeds to deskolemization introducing
new predicate symbols that simulate Skolem functions. This is in
contrast with our spirit of staying inside the original signature.

% What does Dedukti do with skolem functions?  DM Seems that they
% don't do anything interesting with it yet.

\bibliography{/home/matteo/repo/parsifal/papers/references/master}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

%%  LocalWords:  miniscoping preprocessing deskolemize deskolemized
%%  LocalWords:  deskolemizing prenexing reimplementation Elpi sk DM
%%  LocalWords:  herbrandization utte Tait FO Baaz et al Reger Suda
%%  LocalWords:  wft LKFa Matteo async allCx someE someEx TODO allC
%%  LocalWords:  Prog BDD miniscoped unskolemized Nivelle
%%  LocalWords:  skolemizations
